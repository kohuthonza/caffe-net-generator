layer {
  name: "__TOP__-branch0-bn0"
  type: "BatchNorm"
  bottom: "__IN__"
  top: "__TOP__-branch0-bn0"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch0-signum0"
  type: "Signum"
  bottom: "__TOP__-branch0-bn0"
  top: "__TOP__-branch0-signum0"
}
layer {
  name:  "__TOP__-branch0-drop0"
  type: "Dropout"
  bottom: "__TOP__-branch0-signum0"
  top: "__TOP__-branch0-signum0"
  dropout_param {
    dropout_ratio: __RATIO:0.0__
  }
}
layer {
  name: "__TOP__-branch0-conv0"
  type: "BinaryConvolution"
  bottom: "__TOP__-branch0-signum0"
  top: "__TOP__-branch0-conv0"

  param { lr_mult: __W_LR:1__
       decay_mult: __W_DC:1__}
  param { lr_mult: __B_LR:1__
       decay_mult: 0}
  convolution_param {
    bias_term: __BIAS:true__
    kernel_size: __K_SIZE0:5__
    num_output: __CHANNELS0:16__
    stride: 1
    pad: __PAD0:2__
    group: __GROUP:1__
    weight_filler {
      type: "__W_FILLER:xavier__"
      std: __W_STD:0.001__
    }
    bias_filler {
      type: "constant"
      value: __B_VAL:0__
    }
  }
  binary_convolution_param {
    update_weight_diff: __UPDATE:true__
    scale_weight_diff: __SCALE:true__
  }
}
layer {
  name: "__TOP__-branch0-bn1"
  type: "BatchNorm"
  bottom: "__TOP__-branch0-conv0"
  top: "__TOP__-branch0-bn1"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch0-signum1"
  type: "Signum"
  bottom: "__TOP__-branch0-bn1"
  top: "__TOP__-branch0-signum1"
}

layer {
  name: "__TOP__-branch1-bn0"
  type: "BatchNorm"
  bottom: "__IN__"
  top: "__TOP__-branch1-bn0"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch1-signum0"
  type: "Signum"
  bottom: "__TOP__-branch1-bn0"
  top: "__TOP__-branch1-signum0"
}
layer {
  name:  "__TOP__-branch1-drop0"
  type: "Dropout"
  bottom: "__TOP__-branch1-signum0"
  top: "__TOP__-branch1-signum0"
  dropout_param {
    dropout_ratio: __RATIO:0.0__
  }
}
layer {
  name: "__TOP__-branch1-conv0"
  type: "BinaryConvolution"
  bottom: "__TOP__-branch1-signum0"
  top: "__TOP__-branch1-conv0"

  param { lr_mult: __W_LR:1__
       decay_mult: __W_DC:1__}
  param { lr_mult: __B_LR:1__
       decay_mult: 0}
  convolution_param {
    bias_term: __BIAS:true__
    kernel_size: __K_SIZE1:3__
    num_output: __CHANNELS1:32__
    stride: 1
    pad: __PAD1:1__
    group: __GROUP:1__
    weight_filler {
      type: "__W_FILLER:xavier__"
      std: __W_STD:0.001__
    }
    bias_filler {
      type: "constant"
      value: __B_VAL:0__
    }
  }
  binary_convolution_param {
    update_weight_diff: __UPDATE:true__
    scale_weight_diff: __SCALE:true__
  }
}
layer {
  name: "__TOP__-branch1-bn1"
  type: "BatchNorm"
  bottom: "__TOP__-branch1-conv0"
  top: "__TOP__-branch1-bn1"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch1-signum1"
  type: "Signum"
  bottom: "__TOP__-branch1-bn1"
  top: "__TOP__-branch1-signum1"
}

layer {
  name: "__TOP__-branch2-bn0"
  type: "BatchNorm"
  bottom: "__IN__"
  top: "__TOP__-branch2-bn0"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch2-signum0"
  type: "Signum"
  bottom: "__TOP__-branch2-bn0"
  top: "__TOP__-branch2-signum0"
}
layer {
  name:  "__TOP__-branch2-drop0"
  type: "Dropout"
  bottom: "__TOP__-branch2-signum0"
  top: "__TOP__-branch2-signum0"
  dropout_param {
    dropout_ratio: __RATIO:0.0__
  }
}
layer {
  name: "__TOP__-branch2-conv0"
  type: "BinaryConvolution"
  bottom: "__TOP__-branch2-signum0"
  top: "__TOP__-branch2-conv0"

  param { lr_mult: __W_LR:1__
       decay_mult: __W_DC:1__}
  param { lr_mult: __B_LR:1__
       decay_mult: 0}
  convolution_param {
    bias_term: __BIAS:true__
    kernel_size: __K_SIZE2:1__
    num_output: __CHANNELS2:128__
    stride: 1
    pad: __PAD2:0__
    group: __GROUP:1__
    weight_filler {
      type: "__W_FILLER:xavier__"
      std: __W_STD:0.001__
    }
    bias_filler {
      type: "constant"
      value: __B_VAL:0__
    }
  }
  binary_convolution_param {
    update_weight_diff: __UPDATE:true__
    scale_weight_diff: __SCALE:true__
  }
}
layer {
  name: "__TOP__-branch2-bn1"
  type: "BatchNorm"
  bottom: "__TOP__-branch2-conv0"
  top: "__TOP__-branch2-bn1"
  batch_norm_param {
    moving_average_fraction: 0.992
    eps: 0.0001
    #use_global_stats: __GLOBAL_STATS:false__
  }
  param{lr_mult:0.0}
  param{lr_mult:0.0}
  param{lr_mult:0.0}
}
layer {
  name: "__TOP__-branch2-signum1"
  type: "Signum"
  bottom: "__TOP__-branch2-bn1"
  top: "__TOP__-branch2-signum1"
}

layer {
  name: "__TOP__"
  bottom: "__TOP__-branch0-signum1"
  bottom: "__TOP__-branch1-signum1"
  bottom: "__TOP__-branch2-signum1"
  top: "__TOP__"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
